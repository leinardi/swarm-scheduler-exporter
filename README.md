# Swarm Scheduler Exporter

Prometheus exporter for Docker Swarm focused on **task state visibility**, **accurate desired replicas**, and **operability at scale**.
Fork of (and huge thanks to) **[akerouanton/swarm-tasks-exporter](https://github.com/akerouanton/swarm-tasks-exporter)**.

## ğŸ’¡ What is â€œSwarm Scheduler Exporterâ€?

**Swarm Scheduler Exporter** surfaces what Docker Swarmâ€™s **scheduler** is doing right now, and *why*. It reports **desired replicas** (including accurate
eligibility for `global` services), **live task state** per service (latest per slot), **service update/rollback** state + timestamps, **cluster node**
availability, and now **SLO-friendly service readiness** signals.

## ğŸ“¦ What This Exporter Does

- Watches Swarm **service** and **node** events to keep metrics fresh (resilient reconnect, bounded worker pool).
- Periodically polls **tasks** and aggregates **current** states per service (latest per slot, exhaustive zero-emission).
- Computes **desired replicas** precisely for `global` services (eligible nodes only: status/availability/constraints/platforms).
- Emits **exporter** and **cluster** health metrics for alerting & SLOs.
- Sanitizes and validates **custom labels** for Prometheus compliance and safe cardinality.

## ğŸª¶ Resource usage

Swarm Scheduler Exporter is designed to be lightweight. In typical Docker deployments it can sit around **~25 MiB RAM** when idle (exact usage depends on
platform, Go version, and container runtime settings).

## ğŸ“Š Metrics

All metrics live under the `swarm_` namespace.

### Service-level

- `swarm_service_desired_replicas{stack,service,service_mode,...custom}`
  Desired replicas (**replicated**: configured replicas; **global**: eligible nodes).

- `swarm_task_replicas_state{stack,service,service_mode,state,...custom}`
  **Latest-per-slot** task count by state (always emits zeros for all known states per current service).

- `swarm_service_running_replicas{stack,service,service_mode,...custom}`
  Number of **currently running** tasks per service (latest-per-slot view, same snapshot as `replicas_state`).

- `swarm_service_at_desired{stack,service,service_mode,...custom}`
  `1` if `running_replicas == desired_replicas`, else `0`. Useful for dead-simple SLOs and alerting.

> â„¹ï¸ **Global services with 0 eligible nodes:** `desired_replicas=0`, `running_replicas` usually `0` â‡’ `at_desired=1`.

### Service update/rollback (info-style)

- `swarm_service_update_state_info{stack,service,service_mode,state}` = `1` for the *current* state, else `0`.
  States: `updating`, `completed`, `paused`, `rollback_started`, `rollback_completed`.

- `swarm_service_update_started_timestamp_seconds{...}`
- `swarm_service_update_completed_timestamp_seconds{...}`

### Cluster / node visibility

- `swarm_cluster_nodes_by_state{role,availability,status}`
  Count of nodes by manager/worker, active/pause/drain, and ready/down/â€¦ .

### Exporter self-metrics

- `swarm_exporter_health` â€” `1` healthy / `0` unhealthy.
- `swarm_exporter_build_info{version,commit,date}` â€” `1`.
- `swarm_exporter_polls_total` / `swarm_exporter_poll_errors_total`.
- `swarm_exporter_poll_duration_seconds` (histogram).
- `swarm_exporter_events_reconnects_total`.

### Container-level (opt-in)

If you also run non-Swarm workloads (e.g. plain Docker Compose or standalone containers),
the exporter can expose **container state** metrics when started with `-containers`.

- `swarm_container_state{project,stack,service,container,orchestrator,display_name,state,exit_code}`
  Emits an **info-style one-hot series** per container across all known states.
  Exactly one time series per container has `1`, the rest are `0`.

    - `project` / `service` â€” from Compose labels (`com.docker.compose.*`), if present
    - `stack` â€” from Swarm labels (`com.docker.stack.namespace`), if present
    - `container` â€” sanitized container name
    - `orchestrator` â€” `compose`, `swarm`, or `none`
    - `display_name` â€” friendly name (`stack service` or `stack` if identical)
    - `state` â€” one of
      `created`, `restarting`, `running`, `removing`, `paused`, `exited`, `dead`, `healthy`, `unhealthy`, `health_starting`
    - `exit_code` â€” string exit code (only when `state="exited"`, otherwise empty)

> â„¹ï¸ The exporter inspects only a **bounded subset** of containers per poll:
> running containers with healthchecks (for health state) and exited containers (for exit code).
> Swarm task containers are skipped unless `-containers-include-swarm` is set.

## âœ… Health

- HTTP: `/healthz` responds `200` when the exporter is healthy.
- Metric: `swarm_exporter_health` mirrors health for scraping/alerting.

## ğŸš€ Quick Start

When running the exporter inside a container, it needs permission to talk to the Docker Engine.
On most systems, this means allowing access to the Docker UNIX socket at `/var/run/docker.sock`.

### ğŸ” Finding the correct socket group

Dockerâ€™s socket is owned by a specific group (e.g., `docker` or `root`).
Check the numeric group ID (GID) on your system:

```bash
stat -c %g /var/run/docker.sock
```

Use that GID in the `--group` or `--group-add` flag so the containerâ€™s user
(in the distroless image itâ€™s a nonroot user, UID 65532) can connect to the socket.

If you skip this step, youâ€™ll see errors like:

```
permission denied while trying to connect to the Docker daemon socket
```

> âš ï¸ The GID must be the same on **all Swarm manager nodes** if you use a bind mount for the socket.
> If GIDs differ, use the **TCP/TLS approach** below instead of the socket.

### ğŸ³ Docker (single host)

```bash
docker run --rm \
 -p 8888:8888 \
 -v /var/run/docker.sock:/var/run/docker.sock:ro \
 --group-add 140 \
 ghcr.io/leinardi/swarm-scheduler-exporter:latest \
 -log-format text \
 -log-level warn
```

Replace `140` with the value from `stat -c %g /var/run/docker.sock`.

### ğŸ Swarm service (recommended)

```bash
docker service create \
 --name swarm-scheduler-exporter \
 --mode replicated --replicas 1 \
 --constraint 'node.role == manager' \
 --group 140 \
 --mount type=bind,src=/var/run/docker.sock,dst=/var/run/docker.sock \
 --read-only \
 --mount type=tmpfs,dst=/tmp,tmpfs-size=16m \
 --publish published=8888,target=8888 \
 ghcr.io/leinardi/swarm-scheduler-exporter:latest \
 -poll-delay 10s
```

> â„¹ï¸ **Why the `manager` constraint?**
> Only manager nodes can access cluster-wide service, node, and event data required by the exporter.

### ğŸ” Alternative: TCP/TLS (no socket mount)

If your nodes have mismatched socket GIDs or you prefer not to expose `/var/run/docker.sock`,
you can use Dockerâ€™s authenticated API instead:

```bash
docker service create \
 --name swarm-scheduler-exporter \
 --mode replicated --replicas 1 \
 --constraint 'node.role == manager' \
 --read-only \
 --mount type=tmpfs,dst=/tmp,tmpfs-size=16m \
 --publish published=8888,target=8888 \
 --env DOCKER_HOST=tcp://manager.example.internal:2376 \
 --env DOCKER_TLS_VERIFY=1 \
 --mount type=bind,src=/path/to/certs,dst=/run/certs,ro \
 --env DOCKER_CERT_PATH=/run/certs \
 ghcr.io/leinardi/swarm-scheduler-exporter:latest \
 -poll-delay 10s
```

This avoids group and permission issues, relying instead on proper TLS authentication.

### ğŸ§© Docker Compose Example

A complete Compose setup (replicated mode, manager constraint, and environment hints)
is available at:
[`deployments/docker/docker-compose.yaml`](deployments/docker/docker-compose.yaml)

## âš™ï¸ Configuration

### Flags

```
  -containers
        Expose container state metrics (opt-in).
  -containers-include-swarm
        Include containers belonging to Swarm tasks.
  -help
        Display help message
  -label value
        Name of custom service labels to add to metrics
  -listen-addr string
        IP address and port to bind (default "0.0.0.0:8888")
  -log-format string
        Either json, text or plain (default "text")
  -log-level string
        Either debug, info, warn, error, fatal, panic (default "info")
  -log-time
        Include timestamp in logs
  -poll-delay duration
        How often to poll tasks (Go duration, e.g. 10s, 1m). Minimum 1s. (default 10s)
```

### Environment (Docker client)

- `DOCKER_HOST` â€” Docker daemon URL
- `DOCKER_CERT_PATH` â€” Path to TLS certs
- `DOCKER_TLS_VERIFY` â€” Enable TLS verification (set to `1`)

### Custom label guardrails

- Names are validated & **sanitized** to Prometheus label rules
  (e.g., `app.kubernetes.io/name` â†’ `app_kubernetes_io_name`).
- Duplicate/colliding sanitized names are rejected at startup.
- Max number of custom label keys is bounded (sane default).
- Suspicious **high-cardinality values** log a one-time warning.

## ğŸ”” Example Alerts

Prometheus Alert rule:

```yaml
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Swarm service not at desired replicas (and not just updating)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
- alert: SwarmServiceNotAtDesired
  expr: |
    (swarm_service_at_desired == 0)
    and on (stack, service)
    (swarm_service_update_state_info{state="updating"} == 0)
  for: 5m
  labels:
    severity: warning
    service: swarm
  annotations:
    summary: "Service {{ $labels.stack }}/{{ $labels.service }} not at desired replicas"
    description: |
      The Swarm service {{ $labels.stack }}/{{ $labels.service }} is not running
      at its desired replica count for more than 5 minutes and is not in
      an 'updating' state.
      Check the service tasks, node availability, and recent changes.

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Swarm service in rollback state
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
- alert: SwarmServiceRollbackOngoing
  expr: |
    sum by (stack, service, display_name) (
      swarm_service_update_state_info{state=~"rollback_(started|completed)"}
    ) > 0
  for: 1m
  labels:
    severity: warning
    service: swarm
  annotations:
    summary: "Service rollback detected for {{ $labels.display_name }}"
    description: |
      The Swarm service {{ $labels.display_name }} is in a rollback state
      (rollback_started or rollback_completed).
      Investigate the deployment history and task failures.

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Containers in an unhealthy / failed state (Compose/standalone)
#   - Includes: unhealthy, exited, dead
#   - Excludes: one-shot containers that exited/dead with exit_code=0
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
- alert: ContainerUnhealthy
  expr: |
    (
      sum by (display_name, stack, service) (
        swarm_container_state{
          orchestrator=~"compose|none",
          state=~"unhealthy|exited|dead"
        }
      ) > 0
    )
    UNLESS
    (
      sum by (display_name, stack, service) (
        swarm_container_state{
          orchestrator=~"compose|none",
          state=~"exited|dead",
          exit_code="0"
        }
      ) > 0
    )
  for: 5m
  labels:
    severity: warning
    service: containers
  annotations:
    summary: "Container unhealthy or failed: {{ $labels.display_name }}"
    description: |
      The container {{ $labels.display_name }} is unhealthy, exited, or dead
      (and not a clean one-shot exit with code 0) for more than 5 minutes.
      Check logs and recent changes to this container.

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Swarm cluster node(s) not ready
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
- alert: SwarmClusterNodeNotReady
  expr: swarm_cluster_nodes_by_state{status!="ready"} > 0
  for: 5m
  labels:
    severity: warning
    service: swarm
  annotations:
    summary: "Swarm cluster has node(s) not ready"
    description: |
      One or more Swarm nodes are not in 'ready' status.
      Check 'docker node ls' and node availability / connectivity.
```

## ğŸ§ª Quick checks

- **Metrics**: `curl http://<host>:8888/metrics`
- **Health**: `curl -s -o /dev/null -w "%{http_code}\n" http://<host>:8888/healthz` (200 healthy)

## ğŸ” Example Prometheus scrape config

```yaml
scrape_configs:
  - job_name: 'swarm-scheduler-exporter'
    static_configs:
      - targets: [ 'swarm-manager:8888' ]
```

## ğŸ” Security & Permissions

- Only needs **read-only** access to the Docker API (`/var/run/docker.sock:ro`).
- Must run on a **manager** node in Swarm to receive cluster-wide events and inspect services.
- Avoid exposing the exporter to untrusted networks; it exposes metrics only, but your scrape endpoint should be internal.

## ğŸ›  Addressed vs Original Project

- **Data races**: guarded metadata cache; removed global `nodeCount`; added worker pool; no per-event goroutines.
- **Event resiliency**: reconnect with capped backoff; bounded workers; fixed pointer-to-loop-var; per-worker panic recovery.
- **Series lifecycle**: `replicas_state` now `Reset()`s each publish; exhaustive zero emission per current service; delete series on service remove.
- **Global desired replicas accuracy**: evaluate **eligible nodes** (status/availability/constraints/platforms), not total nodes.
- **Label sanitation & validation**: full Prometheus regex, collision checks, max label keys, high-cardinality warning, rawâ†’sanitized mapping.
- **Operability**: graceful shutdown; `/healthz`; health/build/exporter metrics; quieter default logs; validated `-poll-delay`.
- **Performance**: node snapshot cache; on node events recompute **only** global services; task poll optimized to â€œlatest per slotâ€; worker pool.
- **Metrics namespace**: consistent `swarm_*` names & labels aligned with Prometheus best practices.
- **Service update visibility**: `swarm_service_update_state_info` + timestamps for rollbacks/paused/update flows.
- **SLO helpers**: `swarm_service_running_replicas` and `swarm_service_at_desired` for direct alerting/dashboards.

## ğŸ¤ Contributing

Issues and PRs are welcome! Please run linters and keep changes modular:

- `pre-commit run`
- Keep labels/metrics backward-considerate unless the change is clearly an improvement

## ğŸ™ Acknowledgements

This project stands on the shoulders of **[akerouanton/swarm-tasks-exporter](https://github.com/akerouanton/swarm-tasks-exporter)**.
Thank you for the original implementation and the inspiration to monitor Swarm task health with Prometheus.
